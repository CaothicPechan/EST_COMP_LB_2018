---
title: "Inferencia (frecuentista)"
author: "León Berdichevsky Acosta"
date: "06 de septiembre de 2018"
output: html_document
---

```{r options, echo = FALSE, message=FALSE, error=TRUE}
knitr::opts_chunk$set(
    comment = "#>",
    collapse = TRUE
)
comma <- function(x) format(x, digits = 2, big.mark = ",")
options(digits=3)

library(tidyverse)
library(magrittr)
library(gridExtra)
theme_set(theme_minimal())
```

Estas notas están basadas en el libro  [An Introduction to the Bootstrap de
Effron y Tibshirani](http://www.amazon.com/Introduction-Bootstrap-Monographs-Statistics-Probability/dp/0412042312) y abordan los siguientes temas:

* Muestras aleatorias  
* El principio del _plug-in_  
* Bootstrap no paramétrico  

<!-- la inferencia estadística se ocupa de aprender de la experiencia: 
observamos una muestra aleatoria x y queremos inferir propiedades de la 
población que produjo la muestra. Probabilidad va en la dirección contraria:
de la composicion de una pob deducimos las propiedades de una muestra aleatoria
x -->


## Muestras aleatorias
Supongamos que tenemos una población finita o **universo** $U$, conformado
por **unidades individuales** $U_1,U_2,...,U_N$ caracterizadas por una **unidad observacional** (individuos de género masculino, etc.). Cada unidad individual tiene la misma probabilidad de ser seleccionada en una **extracción** aleatoria. Las unidades individuales
$\{U_i\}$ tienen propiedades que nos gustaría aprender (opinión política, etc.). 
Debido a que es muy difícil o simplemente imposible examinar cada unidad en $U$ seleccionamos una 
muestra aleatoria.

<div class="caja">
Una **muestra aleatoria** de tamaño $n$ se define como una colección de $n$
unidades $u_1,...,u_n$ seleccionadas aleatoriamente de un universo $U$ compuesto por $N$ unidades, con $N>n$.  
</div>

En principio el proceso de muestreo es como sigue:

1. Seleccionamos $n$ números enteros de manera independiente $j_1,...,j_n$ (con 
probabilidad $1/N$), cada uno de ellos asociado a un número entre $1$ y $N$.

2. Los enteros determinan las unidades que seleccionamos: 
$u_1=U_{j_1},u_2=U_{j_2},...,u_n=U_{j_n}$.

En la práctica el proceso de selección suele ser más complicado y la
definición de la población $U$ suele ser deficiente; sin embargo, el marco
conceptual sigue siendo útil para entender la inferencia estadística.

En específico, el proceso de muestreo descrito anteriormente es equivalente a una serie de $n$ repeticiones independientes de un experimento aleatorio (o ensayos) en los que cada unidad seleccionada corresponde a un resultado (*output*) posible de cada experimento, y el espacio de resultados está dado por el universo $U$. Ésto nos permite interpretar las **frecuencias observadas** de las unidad individuales en la muestra utilizando la definición frecuentista de probabilidad: en términos de frecuencias relativas.

Observación: Nuestra definición de muestra aleatoria permite que una unidad 
particular $U_i$ aparezca más de una vez, podríamos evitar esto si realizamos
un **muestreo sin remplazo**; sin embargo, es un poco más sencillo permitir 
repeticiones y si el tamaño de la muestra $n$ es mucho más pequeño que la 
población $N$, la probabilidad de muestrear la misma unidad más de una vez
es pequña.

Una vez que se selecciona una muestra aleatoria $u_1,...,u_n$ obtenemos una o más
**medidas** de interés $x_i$ (o **características**, o **variables**) para cada unidad $u_i$. Cada medida está descrita por una variable aleatoria. Por lo que el conjunto de medidas $x_i$ para cada unidad $u_i$ corresponde a un conjunto de valores, uno por cada variables aleatorias.  

Los **datos observados** son la colección
de medidas $x_1,...,x_n$, que también denotaremos $\textbf{x} = (x_1,...,x_n)$, asociados a la muestra aleatoria.

También podemos obtener las medidas de interés de cada unidad en la población 
$U_1,U_2,...,U_N$, obteniendo así los valores $X_1,...,X_N$; esto
sería un **censo**, y denotamos al conjunto de mediciones de la población por
$\mathcal{X}$. 

El objetivo de la **inferencia estadística** es expresar lo que hemos 
aprendido de la población $\mathcal{X}$ a partir de los datos observados $\textbf{x}$.

La **Estadística Inferencial** es la rama de la Estadística que se ocupa de la inferencia estadística. Se divide en dos grandes subareas: 

1. **Estimación** (puntual y por intervalos).
2. **Pruebas de Hipótesis**.

En este curso consideraremos únicamente el problema de Estimación. En particular, vamos a usar el **bootstrap** para determinar la precisión con la
que un **estadístico** (ej., media, mediana) calculada de la muestra $x_1,...,x_n$ estima 
la cantidad correspondiente en la población.

**Ejemplo:** Veamos un ejemplo artificial en el cual tomaremos una muestra del universo completo de escuelas primarias de la Ciudad de México:

```{r}
load("data/base_completa.Rdata")
prim <- tbl_df(primaria) %>%
    filter(entidad == "DISTRITO FEDERAL", !is.na(esp.3), !is.na(esp.6)) %>%
    select(clave, turno, tipo = tipo.esc, mun = clave.mun, esp3 = esp.3, 
        esp6 = esp.6) %>%
    mutate(tipo = as.character(tipo))
```

La población consta de `r nrow(prim)` escuelas. Tomamos una muestra de 500 escuelas: 

```{r muestra_enlace}
set.seed(16021)
n <- 500
prim_muestra <- sample_n(prim, n, replace = TRUE)
glimpse(prim_muestra)
```

Para cada escuela en la muestra tenemos dos medidas $x_i$, dadas por el 
promedio de las calificaciones en español de los alumnos de tercero y sexto 
de primaria (prueba ENLACE 2010):
$$x_i=(esp3_i, esp6_i)$$

Este ejemplo es artificial pues contamos con un censo de las escuelas; sin 
embargo, es común contar únicamente con la muestra. La muestra de 500 escuelas tiene una **media estimada** de la primera medida de `r mean(prim_muestra$esp3)`, con un **error estándar estimado** de 
`r sqrt(sum((prim_muestra$esp3 - mean(prim_muestra$esp3)) ^ 2 / n-1)) / sqrt(n)`. 
Debido a que nuestro ejemplo es artificial podemos comparar con la población: 
la **media poblacional** de las `r nrow(prim)` escuelas es `r mean(prim$esp3)`. 

## El principio del _plug-in_
En este contexto, podemos reformular la definición de distribución empírica en términos de la muestra aleatoria. 

<div class="caja">
Dada una muestra 
aleatoria de tamaño $n$ de una distribución de probabilidad $P$, 
la función de **distribución empírica** $P_n$ se define como la distribución que 
asigna probabilidad $1/n$ a cada valor $x_i$ con $i=1,2,..., n$. En otras
palabras, $P_n$ asigna a un conjunto $A$ en el espacio muestral de $x$ la
probabilidad empírica:

$$P_n(A)=\#\{x_i \in A \}/n$$
</div>

Muchos problemas de inferencia estadística involucran la estimación
de alguna característica de una distribución de probabilidad $P$ en base a una 
muestra aleatoria obtenida de $P$. La función de distribución empírica $P_n$ 
es una estimación de la distribución completa $P$, por lo que una manera 
inmediata de estimar alguna característica de $P$ (ej., media, mediana) es calcular la 
característica correspondiente de $P_n$.

**Ejemplo:** Podemos comparar el histograma de la distribución completa con el histograma
de la distribución empírica para el ejemplo de las calificaciones de la 
prueba ENLACE.

```{r distribucion_empirica, fig.width=5, fig.height=4}
claves_muestra <- prim_muestra$clave
prim_long <- prim %>% 
    gather(grado, calif, esp3, esp6) %>%
    mutate(muestra = ifelse(clave %in% claves_muestra, "muestra", "población"))

ggplot(prim_long, aes(x = calif)) +
  geom_histogram(aes(y = ..density..), binwidth = 20, fill = "darkgray") +
  geom_density() +
  facet_grid(grado ~ muestra)
```

Cuando la variable de interés toma pocos valores es fácil interpretar la distribución 
empírica. Supongamos que la medición de las unidades que nos interesa es la 
variable `tipo` de escuela, entonces la distribución empírica de esta variable calculada de la muestra en cuesión es:

```{r dist_empirica_categorica}
table(prim_muestra$tipo) / n
```

Vale la pena notar que cuando la variable toma valores discretos, pasar de la muestra desagregada a la distribución empírica (lista de valores y la proporción que ocurre cada uno de ellos en la muestra) 
no conlleva ninguna pérdida de información: el **vector de frecuencias observadas** $P_n(\textbf{x}):= (P_n(x_1),...,P_n(x_n))$ es un **estadístico suficiente** para la verdadera distribución. Esto quiere 
decir que toda la información de $P$ contenida en el vector de 
observaciones $\textbf{x}$ está también contenida en la distribución empírica $P_n$.

**Nota**: El **teorema de suficiencia** asume que las observaciones $\textbf{x}$ son
una muestra aleatoria de la distribución $P$, este no es siempre el caso, como por ejemplo si se trata de una serie de tiempo.


**Ejemplo: 100 lanzamientos de un dado.** En este ejemplo obtenemos la siguiente distribución empírica:

```{r dado}
dado <- read.table("../05. Repaso de Probabilidad/data/dado.csv", header=TRUE, quote="\"")
n <- length(dado$observado)
n
table(dado$observado) / n
```

En este caso no tenemos un censo, solo contamos con la muestra. 

Cuando aplicamos teoría estadística a problemas reales, es común que las 
respuestas estén dadas en términos de distribuciones de probabilidad. Para resolver este tipo de preguntas, debemos hacer inferencia a partir de la distribución empírica. 


**Ejemplo:** Una pregunta de inferencia que surge de manera natural en este ejemplo es si el dado es justo, esto es, si la distribución que generó esta muestra tiene una distribución 
$P = (1/6, 1/6, 1/6,1/6, 1/6, 1/6)$.

**Ejemplo:** Podemos preguntarnos qué tan correlacionados están los resultados de las pruebas de 
español correspondientes a tercero y sexto grados. Si conocemos la distribución de 
probabilidad conjunta $P$ de estas dos variables aleatorias, la respuesta a esta pregunta es simplemente cuestión de aritmética: el **coeficiente de correlación poblacional** esta dado por:

$$corr(y,z) = \frac{\sum_{j=1}^{N}(Y_j - \mu_y)(Z_j-\mu_z)P_i}
{[\sum_{j=1}^{N}(Y_j - \mu_y)^2P_i \sum_{j=1}^{N}(Z_j - \mu_z)^2P_i]^{1/2}}$$

donde el punto $(Y_j,Z_j)$ son las medidas de la j-ésima unidad de la población de escuelas primarias $\mathcal{X}$, mientras que $\mu_y=\sum Y_j/3311$ y $\mu_z=\sum Z_j/3311$ son las medias poblacionales y $P_j$ es la probabilidad asociada.

```{r grafica_corr, fig.width=4, fig.height=4}
ggplot(prim, aes(x = esp3, y = esp6)) +
  geom_point(alpha = 0.5)
cor(prim$esp3, prim$esp6)
```

Si no tenemos un censo debemos inferir. Podríamos estimar la correlación 
$corr(y,z)$ a través del coeficiente de correlación muestral:
$$\hat{corr}(y,z) = \frac{\sum_{j=1}^{n}(y_j - \hat{\mu}_y)(z_j-\hat{\mu}_z)}
{[\sum_{j=1}^{n}(y_j - \hat{\mu}_y)^2\sum_{j=1}^{n}(z_j - \hat{\mu}_z)^2]^{1/2}}$$

```{r correlacion}
cor(prim_muestra$esp3, prim_muestra$esp6)
```
o cualquier otro **estimador**.

La idea del **Principio del _plug-in_** es realizar estimaciones utilizando únicamente los datos de la muestra; esto es, sin hacer ninguna suposición acerca de la distribución de probabilidad $P$. 

Antes de proseguir con la definición exacta, repasemos dos conceptos importantes: parámetros y 
estadísticos:

<div class='caja'>
Un **parámetro** es una función de la distribución de probabilidad 
$\theta=t(P)$. Un **estadístico** es una función de la 
muestra $\hat{\theta}=s(\textbf{x})$ y por tanto es una variable aleatoria con una distribución $Q$ inducida por $P$ y denominada **distribución muestral**. Un **estimador** es un **estadístico** utilizado para estimar un **parámetro**.
</div>

Por ejemplo, la $corr(x,y)$ es un parámetro de $P$ y $\hat{corr}(x,y)$ es un 
estadístico con base en $\textbf{x}$ y $\textbf{y}$.

Entonces:

<div style="caja">
El **principio del _plug-in_** es un método para estimar parámetros a 
partir de muestras; la estimación _plug-in_ de un parámetro $\theta=t(P)$ se 
define como:
$$\hat{\theta}=t(P_n).$$
</div>

Es decir, estimamos la función $\theta = t(P)$ de la distribución de probabilidad
$P$ utilizando la misma función pero aplicada a la distribución empírica $\hat{\theta}=t(P_n)$.


**Ejemplos de estimaciones _plug-in_:**

* El estimador $\hat{corr}(y,z)$ del ejemplo de las escuelas es un estimador _plug-in_.

* Estimación _plug-in_ de la mediana de las calificaciones de español para tercero de primaria:

```{r mediana_esp3}
median(prim_muestra$esp3)
```

* Estimación de la probabilidad $\theta$ de que la calificación de español de una escuela sea mayor a 700:

$$\theta=\frac{1}{N}\sum_{j=1}^N I_{\{Y_i>700\}}$$

donde $I_{\{\cdot\}} \in \{0,1\}$ es la función indicadora.

Hacemos la estimación _plug-in_ $\hat{\theta}$:

```{r calif_700}
sum(prim_muestra$esp3 > 700) / n
```


El principio del _plug-in_ provee de una estimación puntual de un parámetro, más no contiene información acerca de la **precisión** de la estimación. 

<!--Usaremos la técnica de **bootstrap** para estudiar el sesgo y el error estándar del estimador _plug-in_ $\hat{\theta}=t(P_n)$. La ventaja del bootstrap es que no es necesario hacer ningún supuesto acerca de $P$, éste produce errores estándar y sesgos de manera automática utilizando únicamente la muestra $\textbf{x}$ y sin importar que tan complicada es la función $t(P)$.-->


### Estimadores del error estándar

Los estadísticos como $\hat{\theta}=t(P_n)$ suelen ser el primer paso en el 
análisis de datos, el siguiente paso es investigar la precisión de las 
estimaciones. El error estándar es la manera más común para describir la 
precisión de un estadístico: 

<div class="caja">
El **error estándar** es la estimación de la desviación estándar de un estadístico.
</div>


#### Estimación _plug-in_ del error estándar

**Ejemplo: El error estándar de una media.** Supongamos que $X$ es una variable aleatoria que toma valores en los reales $\Omega_X = \mathbb{R}$ y con 
distribución de probabilidad $P$. Denotamos por $\mu_P$ y $\sigma_P^2$ la 
media y varianza de $P$,

$$E_P(X)=\mu_P,$$ 
$$var_P(X)=E_P[(X-\mu_P)^2]=\sigma_P^2$$

en la notación enfatizamos la dependencia de la media y varianza en la 
distribución $P$. 

Ahora, sea $\textbf{x}=(x_1,...,x_n)$ una muestra aleatoria de $P$, de tamaño $n$. La media de la muestra es $\bar{x}=\sum_{i=1}^nx_i/n$, el cual es el estimador *plug-in* de $\mu_P$. 

La distribución $P$ de $X$ induce una distribución desconocida $Q$ de $\bar{x}$. Se puede desmostrar que el estadístico $\bar{x}$ tiene esperanza y varianza 

$$E_Q[\bar{x}]=\mu_P$$, 

$$var_Q(\bar{x})=\sigma_P^2/n$$
en la notación enfatizamos la dependencia de la media y varianza en la distribución $Q$, inducida por la distribución $P$.

En palabras: la esperanza de $\bar{x}$ es la misma que la esperanza de $X$, pero
la varianza de $\bar{x}$ es $1/n$ veces la varianza de $X$, así que entre
mayor es la $n$ tenemos una mejor estimación de $\mu_P$ dada por $\bar{x}$.

Para calcular la precisión con la que $\bar{x}$ estima $\mu_P$ calculamos el error estándar, que denotamos por $se_Q(\bar{x})$,  el cual es la raíz de la varianza de $\bar{x}$,
$$se_Q(\bar{x}) = [var_Q(\bar{x})]^{1/2}= \sigma_P/ \sqrt{n}.$$

En este punto podemos usar el principio del _plug-in_ para estimar $\sigma_P$, simplemente sustituimos
$P_n$ por $P$ y obtenemos una estimación de $\sigma_P$:
$$\hat{\sigma}=\sigma_{P_n} = \bigg\{\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})^2\bigg\}^{1/2}=\sqrt{var_{P_n}(X)}$$

por lo que la estimación *plug_in* del error estándar de $\bar{x}$ es:
$$\hat{se}(\bar{x})=se_{P_n}(\bar{x})=\sigma_{P_n}/\sqrt{n}=\bigg\{\frac{1}{n^2}\sum_{i=1}^n(x_i-\bar{x})^2\bigg\}^{1/2}$$
en la notación de la primera igualdad enfatizamos que la estimación _plug-in_ de un parámetro de la distribución inducida $Q$ se puede calcular a partir de la muestra $\textbf{x}$; esto es, depende de la distribución empírica $P_n$ de $P$.  

Notemos que usamos el principio del _plug-in_ en dos ocasiones:

1. Primero para 
estimar la esperanza $\mu_P$ mediante $\bar{x}=\mu_{P_n}$.

2. Posteriormente para estimar el error estándar $se_Q(\bar{x})$ mediante $se_{P_n}(\bar{x})$. 

En términos generales, esperamos que $\bar{x}$ 
este a una distancia de $\mu_P$ menor a un error estándar el 68% del tiempo, 
y a menos de 2 errores estándar el 95% del tiempo. Estos porcentajes están 
basados el teorema central del límite que nos dice que bajo ciertas condiciones 
(bastante generales) de $P$ la distribución de $\bar{x}$ se aproximará a una 
distribución normal:
$$\bar{x} \sim Q \approx N(\mu_P,\sigma_P^2/n), \quad n\gg 1$$
En el caso de la
media $\hat{\theta}=\bar{x}$, la aplicación del principio del _plug-in_
para el cálculo de errores estándar es inmediata; sin embargo, hay estadísticos
para los cuáles no es fácil o es imposible calcular de manera exacta el estimador *plug-in* y es en estas situaciones cuando recurrimos técnicas computacionales.


## Bootstrap no paramétrico 

La técnica **bootstrap** es un algoritmo computacional basado en remuestreo para aproximar con precisión arbitraria el estimador *plug-in* del error estándar de un estadístico.

### El estimador bootstrap del error estándar

Supongamos que tenemos una muestra aleatoria $\textbf{x}=(x_1,x_2,...,x_n)$ 
proveniente de una distribución de probabilidad desconocida $P$ y deseamos 
estimar un parámetro $\theta = t(P)$ con base en la muestra. Para esto, 
calculamos una estimación $\hat{\theta}=s(\textbf{x})$ (la estimación puede
ser la estimación _plug-in_ $\hat{\theta}=t(P_n)$ pero también puede ser otra). Entonces podemos
utilizar bootstrap para calcular el error estándar de la estimación.

<div style="background-color:mistyrose;padding:5px;">
<p>
Definimos una **muestra bootstrap** como una muestra aleatoria de tamaño $n$ que
se obtiene de la distribución empírica $P_n$ y la denotamos 
$$\textbf{x}^* = (x_1^*,...,x_n^*).$$
</p>
</div>

La notación de estrella indica que $\textbf{x}^*$ no son los datos $\textbf{x}$
sino una versión de **remuestreo** de $\textbf{x}$.

Otra manera de frasearlo: Los datos bootstrap $x_1^*,...,x_n^*$ son una muestra
aleatoria de tamaño $n$ seleccionada con reemplazo de la población de $n$
objetos $(x_1,...,x_n)$ con distribución $P_n$.. 

Ahora, a cada muestra bootstrap $\textbf{x}^*$ le corresponde una **replicación**
$\hat{\theta}^*=s(\textbf{x}^*).$

La fórmula del estimador *plug-in* del error estándar de un estadístico $\hat{\theta}$ es $se_{P_n}(\hat{\theta})$ y no existe para casi ninguna estimación 
diferente a la media, por lo que recurrimos a la técnica computacional 
bootstrap: el algoritmo funciona seleccionando distintas muestras bootstrap, 
evaluando la replicación bootstrap correspondiente y estimando el error estándar
de $\hat{\theta}$ mediante la desviación estándar empírica de las replicaciones.
El resultado es la **estimación bootstrap del error estándar**, que denotamos
$\hat{se}_B$, donde $B$ es el número de muestras bootstrap utilizadas.

<div style="background-color:mistyrose;padding:5px;">
<p>
#### Algoritmo bootstrap para estimar errores estándar
1. Selecciona $B$ muestras bootstrap independientes: 
$$\textbf{x}^{*1},..., \textbf{x}^{*B}$$.  
2. Evalúa la replicación bootstrap correspondiente a cada muestra bootstrap:
$$\hat{\theta}^*(b)=s(\textbf{x}^{*b}), \quad b=1,2,...,B$$

3. Estima el error estándar $se_P(\hat{\theta})$ usando la desviación estándar
muestral de las $B$ replicaciones:
$$\hat{se}_B = \bigg\{\frac{\sum_{b=1}^B[\hat{\theta}^{*}(b)-\hat{\theta}^*(\cdot)]^2 }{B-1}\bigg\}^{1/2}$$

donde $$\hat{\theta}^*(\cdot)=\sum_{b=1}^B \hat{\theta}^{*}(b)/B$$ es la media muestral de las replicaciones $\{\hat{\theta}^{*}\}$.
</p>
</div>

<!--Notemos que la estimación bootstrap de $se_{P}(\hat{\theta})$, esto es, el error estándar
de un estadístico $\hat{\theta}$, es una estimación _plug-in_ en donde la
distribución empírica $P_n$ toma el lugar de la distribución desconocida $P$ y la replicación $\hat{\theta}^*$ toma el lugar del estadístico $\hat{\theta}$. En otras palabras, la estimación bootstrap de $se_P(\hat{\theta})$ es el error
estándar de $\hat{\theta}$ para conjuntos de datos de tamaño $n$ seleccionados
de manera aleatoria de $P_n$.-->

Conforme el número de replicaciones $B$ aumenta el estimador bootstrap del error estándar se aproxima al estimador _plug-in_ del error estándar: 
$$\hat{se}_B\approx se_{P_n}(\hat{\theta}), \quad B\gg1$$
Este hecho equivale a decir que la desviación estándar empírica se acerca a la 
desviación estándar poblacional de $P_n$ conforme crece el número de muestras $B$. 

<!--La _población_ en este caso es la población de valores $\hat{\theta}^*=s(x^*)$.-->

Al estimador bootstrap $\hat{se}_B$ y al estimador *plug-in* $se_{P_n}(\hat{\theta})$ se les denomina estimadores no paramétricos ya que 
estan basados en $P_n$, el estimador no paramétrico de la distribución poblacional $P$.

**Ejemplo:** Escribimos una función para estimar el error estándar de la media de la calificación de la prueba de español de tercer grado de escuelas primarias de la Ciudad de México utilizando la muestra que consideramos antes. 

Calculamos primero el estimador bootstrap del error estándar:

```{r esp3_media_boot, cache=TRUE}
mediaBoot <- function(x){ 
  # x: muestra
  # n: tamaño de la muestra
  n <- length(x)
  muestra_boot <- sample(x, size = n, replace = TRUE) # Una remuestra
  mean(muestra_boot) # Replicacion bootstrap de theta_gorro
}
# Consideramos B=500 replicaciones
set.seed(8372683)
thetas_boot <- rerun(500, mediaBoot(prim_muestra$esp3)) %>% flatten_dbl()
sd(thetas_boot)
```

```{r, eval=FALSE, echo=FALSE}
prim_boot <- prim_muestra %>% 
    broom::bootstrap(m = 1000) %>% 
    do(tidy(mean(.$esp3))) 
sd(prim_boot$x)
```

Y lo comparamos el estimador *plug-in* del error estándar:

```{r}
se <- function(x) sqrt(sum((x - mean(x)) ^ 2)) / length(x)
se(prim_muestra$esp3)
```

**Nota:** Conforme $B$ aumenta tenemos que $\hat{se}_{B}(\bar{x})\to \{\sum_{i=1}^n(x_i - \bar{x})^2 / n \}^{1/2}$, 
lo cual se demuestra con la ley débil de los grandes números.

**Ejercicio 1:** Considera el coeficiente de correlación muestral
entre la calificación de $y=esp3$ y la de $z=esp6$: 
$\hat{corr}(y,z)=0.9$. ¿Qué tan precisa es esta estimación? 



##### ¿Cuántas replicaciones bootstrap $B$?
La estimación bootstrap ideal es un resultado asintótico $B=\infty$, en el que $\hat{se}_B$ converge a la estimación _plug-in_ $se_{P_n}(\hat{\theta})$. En la práctica, para 
elegir el tamaño de $B$, debemos considerar el hecho de que deseamos las mismas propiedades 
para la estimación de un error esrándar que para cualquier estimación: poco 
sesgo y desviación estándar chica. El sesgo de la estimación bootstrap del 
error estándar suele ser bajo.

<!-- Una respuesta aproximada es en términos del coeficiente de variación de 
$\hat{se}_B$, esto es el cociente de la desviación estándar de $\hat{se}_B$ y su 
valor esperado, la variabilidad adicional de parar en $B$ replicaciones en lugar 
de seguir hasta infiniti se refleja en un incremento en el coeficiente de 
variación
-->

Reglas de dedo (Effron y Tibshirani):

1. Incluso un número chico de replicaciones bootstrap, digamos $B=25$ es 
informativo, y $B=50$ con frecuencia es suficiente para dar una buena 
estimación de $se_{P_n}(\hat{\theta})$.

2. En pocos casos es necesario realizar más de $B=200$ replicaciones cuando 
se busca estimar **error estándar**.

**Ejemplo:** Consideremos varios valores para el número de replicaciones $B$ y las estimaciones bootstrap del error estándar de la media de las calificaciones de la prueba de español de tercer grado correspondientes:
```{r}
seMediaBoot <- function(x, B){
    thetas_boot <- rerun(B, mediaBoot(x)) %>% flatten_dbl()
    sd(thetas_boot)
}

B_muestras <- data_frame(n_sims = c(5, 25, 50, 100, 200, 400, 1000, 1500, 3000)) %>% 
    mutate(est = map_dbl(n_sims, ~seMediaBoot(x = prim_muestra$esp3, B = .)))
B_muestras
```


**Ejemplo: Componentes principales: calificaciones en exámenes.**
Los datos _marks_ (Mardia, Kent y Bibby, 1979) contienen una muestra que consiste en los puntajes de 88 estudiantes en 5 pruebas: mecánica, vectores, álgebra, análisis y estadística.
Cada renglón corresponde a la calificación de un estudiante en cada prueba.

```{r leer_marks}
marks <- read_csv("data/marks.csv")
glimpse(marks)
marks <- select(marks, -id)
head(marks)
```
La muestra $\textbf{x}$ es una base de datos de $88 \times 5$, y donde $i$-ésima fila corresponde a al conjunto de medidas $x_i$ de la unidad $u_i$.

Un análisis de componentes principales sería como sigue:
```{r pc, fig.height=4, fig.width=4}
pc_marks <- princomp(marks)
summary(pc_marks)
loadings(pc_marks)
plot(pc_marks, type = "lines")
```


```{r}
biplot(pc_marks)
```

Los cálculos de un análisis de componentes principales involucran la matriz de 
covarianzas empírica $G$ (estimaciones _plug-in_)

$$G_{jk} = \frac{1}{88}\sum_{i=1}^88(x_{ij}-\bar{x_j})(x_{ik}-\bar{x_k})$$

para $j,k=1,2,3,4,5$, y donde $\bar{x_j} = \sum_{i=1}^88 x_{ij} / 88$ (la media 
de la $i$-ésima columna). La matriz de covarianzas empírica es:
```{r}
G <- cov(marks) * 87 / 88
G
```

Los **pesos** $\hat{\lambda}_i$ y las **componentes principales** $\hat{\nu}_i$ estimadas son los valores y vectores propios de la matriz de covarianzas $G$; estos se calculan a través de una 
serie de de manipulaciones algebraicas que requieren cálculos del orden de $p^3$
(cuando $G$ es una matriz de tamaño $p\times p$):
```{r}
eigen_G <- eigen(G)
lambda <- eigen_G$values
v <- eigen_G$vectors
lambda
v
```


Nos interesa estimar el parámetro 
$$\theta=\frac{\lambda_1}{\sum_{i=1}^5\lambda_i}$$
el cual mide el porcentaje de la varianza explicada por la primer componente principal $\nu_1$.
También deseamos cuantificar la estimación mediante el error estándar del estimador \hat{\theta}. 

La estimación *plug-in* es
$$\hat{\theta}=\frac{\hat{\lambda}_1}{\sum_{i=1}^5\hat{\lambda}_i}$$
y está dada por:
```{r}
theta_hat <- lambda[1]/sum(lambda)
theta_hat
```

¿Qué tan preciso es  $\hat{\theta}$? Para calcular el error estándar de $\hat{\theta}$ tenemos que poder calcular una réplica $\hat{\theta}^*$ para una muestra bootstrap $\textbf{x}^*$; en esta caso una
muestra bootstrap es una base de datos de $88 \times 5$.

```{r}
pc_boot <- function(){
    muestra_boot <- sample_n(marks, size = 88, replace = TRUE)
    G <- cov(muestra_boot) * 87 / 88 
    eigen_G <- eigen(G)
    theta_hat <- eigen_G$values[1] / sum(eigen_G$values)
}
# Consideramos B=1000 replicaciones
B <- 1000
thetas_boot <- rerun(B, pc_boot()) %>% flatten_dbl()
```

Veamos un histograma de las replicaciones de  $\hat{\theta}$:
```{r pc_hist, fig.height=4, fig.width=4}
ggplot(data_frame(theta = thetas_boot)) +
    geom_histogram(aes(x = theta, y = ..density..), binwidth = 0.02, fill = "gray40") + 
    geom_vline(aes(xintercept = mean(theta)), color = "red") +
    labs(x = expression(hat(theta)^"*"), y = "")
```

Podemos calcular la media
```{r}
mean(thetas_boot)
```

y la desviación estándar, la cual nos da la estimación bootstrap del error estándar de $\hat{\theta}$:
```{r}
theta_se <- sd(thetas_boot)
theta_se
```



**Ejercicio 2:** Las componentes principales estimadas $\hat{v}_i$ son estadísticos. El vector propio estimado $\hat{v}_1$ corresponde al mayor valor propio estimado  $\hat{\lambda}_1$ y se conoce como primera componente de $G$. Usa bootstrap para dar una medición de su variabilidad.

<!--***
#### Mejorando el código
El código que usamos para el ejemplo de componentes principales arriba es un 
poco lento comparemos el tiempo que tarda con una alternativa usando la función 
`bootstrap()` del paquete broom.

Usamos `system.time()` para comparar tiempos de ejecución, esta función calcula 
el tiempo en segundos que toma ejecutar una expresión (si hay un error, regresa 
el tiempo hasta que ocurre el error):

```{r alternativa_bootstrap, echo=FALSE, eval = FALSE}
library(magrittr)
library(broom)

# system.time calcula el timepo en segundos que toma ejecutar una expresión 
# (si hay un error, regresa el tiempo hasta que ocurre el error):
system.time({
    pc_boot <- function(){
        muestra_boot <- sample_n(marks, size = 88, replace = TRUE)
        G <- cov(muestra_boot) * 87 / 88 
        eigen_G <- eigen(G)
        theta_hat <- eigen_G$values[1] / sum(eigen_G$values)
    }
    B <- 500
    thetas_boot <- rerun(B, pcBoot()) %>% flatten_dbl()
})

system.time({
    prim_boot <- marks %>% 
        broom::bootstrap(m = B) %>% 
        do(cov = cov(.) * 87 / 88) %$%
        map(cov, ~eigen(.)$values) %>% 
        map_dbl(~.[1]/sum(.))
})
```

La función system.time supone que sabes donde buscar, es decir, que expresiones 
debes evaluar, una función que puede ser más útil cuando uno desconoce cuál es 
la función que alenta un programa es `profvis()` (paquete `profvis).

```{r}
library(profvis)

profvis({
    pcBoot <- function(){
        muestra_boot <- sample_n(marks, size = 88, replace = TRUE)
        G <- cov(muestra_boot) * 87 / 88 
        eigen_G <- eigen(G)
        theta_hat <- eigen_G$values[1] / sum(eigen_G$values)
    }
    B <- 500
    thetas_boot_p <- rerun(B, pcBoot())
})
```

`profvis()` utiliza a su vez la función `Rprof()` de R base, este es un 
perfilador de muestreo que registra cambios en la pila de funciones, funciona 
tomando muestras a intervalos regulares y tabula cuánto tiempo se lleva en cada 
función.-->


### Otras estructuras de datos

Introdujimos el bootstrap en el contexto de muestras aleatorias, esto es,
suponiendo que las observaciones son independientes; en este escenario basta con
aproximar la distribución desconocida $P$ usando la distribución empírica $P_n$, 
y el cálculo de los estadísticos es inmediato. Hay casos en los que el mecanismo
que generó los datos es más complicado, por ejemplo, cuando tenemos dos muestras, en diseños de encuestas complejas o en series de tiempo.

#### Dos muestras
Podemos pensar en algún modelo probabilístico $P$ como compuesto por dos poblaciones con distribuciones de probabilidad independientes $G$ y $Q$. En este caso, las observaciones de cada grupo provienen de distribuciones distintas y el método bootstrap debe 
tomar en cuenta esto al generar las muestras bootstrap.

**Ejemplo.** Se realiza un experimento en el que se seleccionan 7 ratones de manera aleatoria
de un total de 16 ratones. A los siete seleccionados se les suministra un 
tratamiento mientras que los restantes formarán el grupo de control. El objetivo
del tratamiento es prolongar la supervivencia de los ratones. La siguiente tabla
muestra el tiempo de supervivencia en días después de suministrar el tratamiento a los ratones del grupo de tratamiento y un placebo a los ratones del grupo de control.

Grupo  |     Datos   |     Tamaño de muestra
-------|-------------|---------------------------
Tratamiento | 94, 197, 16, 38, 99, 141, 23 | 7
Control     | 52, 104, 146, 10, 51, 30, 40, 27, 46 | 9

En este caso podemos pensar en la población del grupo de tratamiento y en la población del grupo de control como independientes. 

**Ejercicio 3:** Determine si el tratamiento prolongó la supervivencia de los ratones que recibieron el tratamiento siguiendo los siguientes pasos:

1. Usa las medias de las muestras para determinar si hay 
diferencias en los grupos; esto es, calcula $\bar{x}-\bar{y}$.
2. Estima el error estándar de la diferencia usando bootstrap. 
3. Corrobora tus resultados del punto anterior usando que 
$\hat{se}(\bar{x}-\bar{y})=\sqrt{\hat{se}(\bar{x})^2 + \hat{se}(\bar{y})^2}$ 
4. ¿Dirías que el tratamiento incrementó la supervivencia de los ratones que recibieron el tratamiento?
5. Supongamos que deseamos comparar los grupos usando las medianas en lugar 
de las medias:
    i. Estima la diferencia de las medias.
    ii. Usa bootstrap para estimar el error estándar de la diferencia. 
    iii. ¿Cuál es tu conclusión de los resultados?


#### Estructuras de datos complejas

La necesidad de estimaciones confiables junto con el uso eficiente de recursos
conllevan a diseños de muestras complejas. Estos diseños típicamente usan las
siguientes técnicas: muestreo sin reemplazo de una población finita, muestreo
sistemático, estratificación, conglomerados, ajustes a no-respuesta, 
postestratificación. Como consecuencia, los valores de la muestra suelen no ser
independientes.

**Ejemplo: Muestreo de encuestas.** La complejidad de los diseños de encuestas conlleva a que el cálculo de errores estándar sea muy complicado, para atacar este problema hay dos técnicas básicas:

1. un enfoque analítico usando linearización, 
2. Métodos de remuestreo como bootstrap. 

El incremento en el poder de cómputo ha favorecido los métodos de
remuestreo pues la linearización requiere del desarrollo de una fórmula para 
cada estimación y supuestos adicionales para simplificar.

Un ejemplo se puede encontrar en el siguiente artículo de 1988 de [Rao y Wu](http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1988.10478591#.VBjm8y5dV8w), en donde los autores propusieron un método de bootstrap para diseños estratificados multietápicos con reemplazo de Unidades Primarias de Muestreo (UPM).


### Intervalos de Confianza

Hasta ahora hemos discutido la idea detrás del bootstrap y como se puede usar 
para estimar el error estándar de un estadístico $\hat{\theta}$. Sin embargo, es difícil interpretar los errores estándar en términos del parámetro a estimar $\theta$. 

Los errores estándar suelen usarse para construir intervalos de confianza. 

Un **intervalo de confianza** es un tipo de estimación por intervalos calculado a partir del estadístico $\hat{\theta}$ y tiene asociado un **nivel de confianza** o **coeficiente de confianza** $\gamma = 1-2\alpha$ con $\gamma \in (0,1)$. Una vez especificado el nivel de confianza $\gamma$, el intervalo de confianza del parámetro $\theta$ es un intervalo con dos extremos aleatorios (dependen de la muestra) $(\theta_{\alpha}(P_n),\theta_{1-\alpha}(P_n))$ tal que:

$$P\{\theta_{\alpha} < \theta < \theta_{1-\alpha}\} = \gamma$$
esto es, la probabilidad de que el intervalo de confianza calculado con la muestra contenga al verdadero valor del parámetro $\theta$ es $\gamma$.

Usualmente, como veremos en un momento, para el cálculo de intervalos de confianza se asume que el estadístico $\hat{\theta}$ tiene una distribución normal. Sin embargo, ya que el bootstrap se puede usar para estimar la función de distribución de $\hat{\theta}$, no es necesario hacer supuestos distribucionales para $\hat{\theta}$; podemos estimar la distribución como parte del proceso de construir intervalos de confianza.

Para estudiar métodos de construcción de intervalos de confianza usaremos el siguiente ejemplo:

**Ejemplo:** supongamos que queremos estimar la asimetría de una base de datos que consta de
799 tiempos de espera entre pulsasiones de un nervio (Cox, Lewis 1976). El estimador *plug-in* de la asimetría es:

$$\hat{\theta} = t(P_n) =\frac{1/n \sum_{i=1}^n(x_i-\hat{\mu})^3}{\hat{\sigma}^3}$$


```{r}
nerve <- read_delim("data/nerve.txt", "\t", escape_double = FALSE, 
    col_names = FALSE, trim_ws = TRUE)
nerve_long <- tidyr::gather(nerve, col, val, X1:X6) %>%
    filter(!is.na(val))

skewness <- function(x){
    n <- length(x)
    1 / n * sum((x - mean(x)) ^ 3) / sd(x) ^ 3 
}
# Calculamos el estadístico
theta_hat <- skewness(nerve_long$val)
theta_hat
# Creamos una muestre bootstrap y calculamos una réplica del estadístico
skewness_boot <- function(x, n){
  x_boot <- sample(x, n, replace = TRUE)
  skewness(x_boot)
}
# Calculamos B=2,000 réplicas
B <- 2000
skewness <- rerun(B, skewness_boot(nerve_long$val, length(nerve_long$val))) %>% 
  flatten_dbl()
# El estimador bootstrap del error estándar de la asimetría es
sd(skewness)

```

1. El **Intervalo Normal** o **Intervalo Estándar**: Si el estadístico $\hat{\theta}$ tiene una distribución normal estándar, entonces el intervalo de confianza con un nivel de confianza de $\gamma = 1-2\alpha$ es 

$$(\theta_{\alpha},\theta_{1-\alpha})=(\hat{\theta}- z^{(1-\alpha)}\cdot se_Q(\hat{\theta}), \hat{\theta}-z^{(\alpha)}\cdot se_Q(\hat{\theta}))$$
donde $z^{(1-\alpha)}=-z^{(\alpha)}$ es el cuantil de orden $1-\alpha$ de la distribución Normal estándar. Si $\hat{\theta}$ no tiene una distribución normal estándar pero su distribución se puede aproximar por una distribución normal estándar entonces el intervalo normal es una aproximación al intervalo de confianza. 

Ya que desconocemos la distribución de probabilidad $Q$ del estadístico $\hat{\theta}$, el intervalo de confianza normal se aproxima mediante:
$$(\hat{\theta}- z^{(1-\alpha)}\cdot se_Q(\hat{\theta}), \hat{\theta}-z^{(\alpha)}\cdot se_Q(\hat{\theta})) \approx (\hat{\theta}- z^{(1-\alpha)}\cdot \hat{se}(\hat{\theta}), \hat{\theta}-z^{(\alpha)}\cdot \hat{se}(\hat{\theta}))$$

Si elejimos un nivel de confianza del $95%$ obtenemos los siguientes extremos del intervalo:
```{r, results="hold"}
li_normal <- round(theta_hat - 1.96 * sd(skewness), 2)
li_normal
ls_normal <- round(theta_hat + 1.96 * sd(skewness), 2)
ls_normal
```

Veamos un histograma de las replicaciones bootstrap de $\hat{\theta}*$

```{r, fig.width=8.5, fig.height=4}
nerve_skewness <- data_frame(skewness)
hist_nerve <- ggplot(nerve_skewness, aes(x = skewness)) + 
        geom_histogram(binwidth = 0.05, fill = "gray30") +
            geom_vline(xintercept = c(li_normal, ls_normal, theta_hat), 
            color = c("black", "black", "red"), alpha = 0.5)
```

Podemos hacer una prueba gráfica de normalidad.
```{r, fig.width=8.5, fig.height=4}
qq_nerve <- ggplot(nerve_skewness) +
  geom_abline(color = "red", alpha = 0.5) +
  stat_qq(aes(sample = skewness), dparams = list(mean = mean(skewness), sd = sd(skewness))) 

grid.arrange(hist_nerve, qq_nerve, ncol = 2, newpage = FALSE)
```
Vemos que el supuesto de normalidad parece razonable.

Veamos como se comparan los cuantiles de la estimación de la distribución de 
$\hat{\theta}$ con los cuantiles de una normal:

```{r, results="hold"}
comma(q_kurt <- quantile(skewness, probs = c(0.025, 0.05, 0.1, 0.9, 0.95, 0.975)))
comma(qnorm(p = c(0.025, 0.05, 0.1, 0.9, 0.95, 0.975), mean = theta_hat, 
  sd = sd(skewness)))
```

Este intervalo no es preciso cuando $\hat{\theta}$ no se distribuye aproximadamente Normal. Por lo que el último análisis sugiere usar los cuantiles del histograma bootstrap para definir los 
límites de los intervalos de confianza:

2. **Intervalo Percentil**: Sea $Q$ la distribución de probabilida de $\hat{\theta}$ y denotemos por $G$ la función de distribución acumulada. El intervalo percentil de $1-2\alpha$ se define por los 
percentiles $\alpha$ y $1-\alpha$ de $G$: 
$$(\theta_{\alpha},\theta_{1-\alpha}) = (G^{-1}(\alpha), G^{-1}(1-\alpha))$$
La distribución bootstrap es la distribución empírica de $Q$. Por lo tanto, podemos aproximar $G$ por la distribución acumulada bootstrap $G^{-1}(\alpha) \approx \hat{\theta}_B^*(\alpha)$, esto es, el percentil 
$100\cdot \alpha$ de la distribución bootstrap, por lo que podemos aproximar el
intervalo percentil mediante el intervalo bootstrap: 
$$(G^{-1}(\alpha), G^{-1}(1-\alpha))\approx(\hat{\theta}_B^*(\alpha),\hat{\theta}_B^*(1-\alpha))$$
En la situación bootstrap _ideal_, 
donde el número de replicaciones bootstrap $B$ es infinito, el intrvalo bootstrap converge al intervalo percentil.

<div style="caja">
El algoritmo para la aproximación bootstrap del intervalo percentil es:

+ Generamos B muestras bootstrap independientes $\textbf{x}^{*1},..., \textbf{x}^{*B}$ y calculamos las replicaciones $\hat{\theta}^{*b}=s(x^{*b}).$  

+ Sea $\hat{\theta}^{*}_B(\alpha)$ el percentil $100\cdot\alpha$ de la 
distribución empírica de $\hat{\theta}$, y sea $\hat{\theta}^{*}_B(1-\alpha)$
el correspondiente percentil $100\cdot (1-\alpha)$, escribimos el intervalo
de confianza con $1-2\alpha$ como 
$$(\theta_{\alpha},\theta_{1-\alpha})\approx(\hat{\theta}^*_B(\alpha),\hat{\theta}^*_B(1-\alpha))$$
</div>

**Ejemplo:** En nuestro ejemplo de la asimetría, los cuantiles muestrales son:
```{r, fig.width=4, fig.height=4}
ggplot(arrange(nerve_skewness, skewness)) + 
    stat_ecdf(aes(x = skewness)) + 
    geom_segment(data = data_frame(x = c(-Inf, -Inf, q_kurt[c(1, 6)]), 
        xend = q_kurt[c(1, 6, 1, 6)], y = c(0.025, 0.975, 0, 0), 
        yend = c(0.025, 0.975, 0.025, 0.975)), aes(x = x, xend = xend, y = y, 
        yend = yend), color = "red", size = 0.4, alpha = 0.5) + 
  labs(x = "Cuantiles muestrales", y = "ecdf")
  
```

Los extremos del intervalo bootstrap son:
```{r, results = "hold"}
ls_per <- round(quantile(skewness, prob = 0.975), 2)
li_per <- round(quantile(skewness, prob = 0.025), 2)
stringr::str_c(li_normal, ls_normal, sep = ",")
stringr::str_c(li_per, ls_per, sep = ",")
```

Si la distribución de $\hat{\theta}$ es aproximadamente normal, entonces 
los intervalos normales y de percentiles serán similares. El teorema del 
límite central nos dice que conforme el tamaño de la muestra $n$ se acerca a infinito el histograma 
bootstrap adquirirá una forma similar a la normal; sin embargo, cuando el 
tamaño de muestra es chico habrá diferencias.

**Ejemplo:** Con el fin de comparar los intervalos normal y percentil creamos un ejemplo de simulación 
(ejemplo tomado de Effron y Tibshirani), generamos una muestra de tamaño 10 de una 
distribución normal estándar.

Supongamos que el parámetro de interés es $\theta=e^{\mu}$ donde $\mu$ es la media poblacional.

```{r, fig.width=8.5, fig.height=4}
set.seed(137612)
# Muestra de una distribución normal estándar de tamaña n=10
x <- rnorm(10)
# Creamos una muestra bootstrap y calculamos la replicación 
boot_sim_exp <- function(){
  x_boot <- sample(x, size = 10, replace = TRUE)
  exp(mean(x_boot))
}
# Consideramos B=1,000 replicaciones
theta_boot <- rerun(1000, boot_sim_exp()) %>% flatten_dbl()
theta_boot_df <- data_frame(theta_boot)
# Creamos el histograma boostrap (replicaciones)
hist_emu <- ggplot(theta_boot_df, aes(x = theta_boot)) +
    geom_histogram(fill = "gray30", binwidth = 0.08) 
# Hacemos la prueba gráfica de normalidad QQ
qq_emu <- ggplot(theta_boot_df) +
    geom_abline(color = "red", alpha = 0.5) +
    stat_qq(aes(sample = theta_boot), 
        dparams = list(mean = mean(theta_boot), sd = sd(theta_boot))) 

grid.arrange(hist_emu, qq_emu, ncol = 2, newpage = FALSE)
```

La distribución empírica de $\hat{\theta}^*$ es asimétrica, por lo que no
esperamos que coincidan los intervalos.

```{r}
# Normal
round(exp(mean(x)) - 1.96 * sd(theta_boot), 2)
round(exp(mean(x)) + 1.96 * sd(theta_boot), 2)

#Percentil
round(quantile(theta_boot, prob = 0.025), 2)
round(quantile(theta_boot, prob = 0.975), 2)
```

La inspección del histograma deja claro que la aproximación normal no es
conveniente en este caso, veamos que ocurre cuando aplicamos la transformación
logarítmica.

```{r, fig.width=8.5, fig.height=4}
hist_log <- ggplot(data_frame(theta_boot), aes(x = log(theta_boot))) +
  geom_histogram(fill = "gray30", binwidth = 0.08) 
qq_log <- ggplot(data_frame(theta_boot)) +
    geom_abline(color = "red", alpha = 0.5) +
    stat_qq(aes(sample = log(theta_boot)), 
        dparams = list(mean = mean(log(theta_boot)), sd = sd(log(theta_boot)))) 

grid.arrange(hist_log, qq_log, ncol = 2, newpage = FALSE)
```

Y los intervalos se comparan:
```{r}
# Normal
round(mean(x) - 1.96 * sd(log(theta_boot)), 2)
round(mean(x) + 1.96 * sd(log(theta_boot)), 2)

#Percentil
round(quantile(log(theta_boot), prob = 0.025), 2)
round(quantile(log(theta_boot), prob = 0.975), 2)
```

La transformación logarítmica convierte la distribución de $\hat{\theta}$ en 
normal y por tanto los intervalos de $\hat{\phi}=log(\hat{\theta})$ son
similares. La forma normal no es sorprendente pues  $\hat{\phi}=\bar{x}$. 

Si mapeamos los intervalos normales calculados para $log(\hat{\theta})$ de 
regreso a la escala de $\theta$ obtenemos intervalos similares a los calculados
para $\hat{\theta}$ usando percentiles:

```{r}
exp(round(mean(x) - 1.96 * sd(log(theta_boot)), 2))
exp(round(mean(x) + 1.96 * sd(log(theta_boot)), 2))
```

Podemos ver que el método de aplicar una transformación al estadístico original $\hat{\theta}$, calcular intervalos normales y aplicar la transformación inversa para volver a la escala
original genera intervalos de confianza atractivos. El problema con este 
método es que requiere que conozcamos la transformación adecuada para cada 
parámetro. 

Por otra parte, podemos pensar en el método del percentil como un 
algoritmo que incorpora la transformación de manera automática, por lo que los intervalos percentiles arrojan directamente intervalos de confianza atractivos.

<div class="caja>
**Lema.** Supongamos que la transformación $\hat{\phi}=m(\hat{\theta})$ 
normaliza la distribución de $\hat{\theta}$ de manera perfecta, 
$$\hat{\phi} \sim N(\phi, c^2)$$
para alguna desviación estándar constante $c$. Entonces el intervalo de percentil basado
en $\hat{\theta}$ es igual a 
$$(m^{-1} (\hat{\phi}-z^{(1-\alpha)}c), m^{-1}(\hat{\phi}-z^{(\alpha)}c))$$
</div>

Existen otras alternativas al método del percentil y cubren otras fallas del 
intervalo normal. Por ejemplo, hay ocasiones en que $\hat{\theta}$ tiene una
distribución normal sesgada:
$$\hat{\theta} \sim N(\theta + sesgo, se^2(\hat{\theta}))$$

en este caso no existe una transformación $m(\theta)$ que _arregle_ el intervalo.

**Intervalos acelerados y corregidos por sesgo**. Esta es una versión mejorada
del intervalo de percentil, la denotamos $BC_{a}$ (bias-corrected and 
accelerated).

**Ejemplo:** Usaremos un ejemplo de Effron y Tibshirani: los datos constan de los resultados 
en dos pruebas espaciales $(A, B)$ de 26 niños con algún problema neurológico. Supongamos
que queremos calcular un intervalo de confianza de 90\% para $\theta=var(A)$.
El estimador plugin es:
$$\hat{\theta}=\sum_{i=1}^n(A_i-\bar{A})^2/n$$
notemos que el estimador _plug-in_ es ligeramente menor que el estimador
usual insesgado:
$$\hat{\theta}=\sum_{i=1}^n(A_i-\bar{A})^2/(n-1)$$

```{r spatial, fig.width=3.5, fig.height=3.5}
library(bootstrap)
data(spatial)
ggplot(spatial) +
    geom_point(aes(A, B))

sum((spatial$A - mean(spatial$A)) ^ 2) / nrow(spatial)
sum((spatial$A - mean(spatial$A)) ^ 2) / (nrow(spatial) - 1)
```

El método $BC_{a}$ corrige el sesgo del estimador *plug-in* (y de cualquier otro estimador) de manera automática, lo cuál es una 
de sus prinicipales ventajas comparado con el método del percentil.

<div class="caja>
Los extremos en los intervalos $BC_{a}$ están dados por percentiles de la
distribución bootstrap, los percentiles usados dependen de dos números $\hat{a}$
y $\hat{z}_0$, que se denominan la aceleración y la corrección del sesgo:
$$(\hat{\theta}_{\alpha}, \hat{\theta}_{1-\alpha})\approx(\hat{\theta}_B^*(\alpha_1), \hat{\theta}_B^*(\alpha_2))$$ 
donde 
$$\alpha_1= \Phi\bigg(\hat{z}_0 + \frac{\hat{z}_0 + z^{(\alpha)}}{1- \hat{a}(\hat{z}_0 + z^{(\alpha)})}\bigg)$$
$$\alpha_2= \Phi\bigg(\hat{z}_0 + \frac{\hat{z}_0 + z^{(1-\alpha)}}{1- \hat{a}(\hat{z}_0 + z^{(1-\alpha)})}\bigg)$$
$\Phi$ es la función de distribución acumulada de la distribución normal estándar
y $z^{\alpha}$ es el percentil $100 \cdot \alpha$ de una distribución normal
estándar. 
</div>

Notemos que si $\hat{a}$ y $\hat{z}_0$ son cero entonces $\alpha_1=\alpha$ y 
y $\alpha_2=1-\alpha$, obteniendo así los intervalos de percentiles.

El valor de la corrección por sesgo $\hat{z}_0$ se obtiene de la 
propoción de de replicaciones bootstrap menores a la estimación original 
$\hat{\theta}$, 

$$\Phi^{-1}\bigg(\frac{\#\{\hat{\theta}^*(b) < \hat{\theta} \} }{B} \bigg)$$

a grandes razgos $\hat{z}_0$ mide la mediana del sesgo de $\hat{\theta}^*$, esto 
es, la discrepancia entre la mediana de $\hat{\theta}^*$ y $\hat{\theta}$ en 
unidades normales.

Por su parte la aceleración $\hat{a}$ se refiere a la tasa de cambio del error 
estándar de $\hat{\theta}$ respecto al verdadero valor del parámetro $\theta$. 
La aproximación normal estándar usual $\hat{\theta} \approx N(\theta, c^2)$ supone que 
el error estándar de $\hat{\theta}$ es el mismo para toda $\hat{\theta}$. esto 
puede ser poco realista, en nuestro ejemplo, donde $\hat{\theta}$ es la varianza de $A$,
si los datos provienen de una normal entonces $se(\hat{\theta})$ depende de $\hat{\theta}$. 

Una manera de calcular $\hat{a}$ es

$$\hat{a}=\frac{\sum_{i=1}^n (\hat{\theta}(\cdot) - \hat{\theta}(i))^3}{6\{\sum_{i=1}^n (\hat{\theta}(\cdot) - \hat{\theta}(i))^2\}^{3/2}}$$

Los intervalos $BC_{a}$ tienen 2 ventajas teóricas: 

1) Respetan transformaciones: esto nos dice que los extremos del intervalo se transforman de manera adecuada si cambiamos el parámetro de interés por una función del mismo. 

2) Su precisión: los intervalos $BC_{a}$ tienen precisión de segundo orden, esto es, los errores
de cobertura se van a cero a una tasa de $1/n$ (los intervalos estándar y de 
percentiles tienen precisión de primer orden).

Los intervalos $BC_{a}$ están implementados en el paquete `boot` (`boot.ci`) y 
en el paquete `bootstrap` (`bcanon`). 

La desventaja de los intervalos $BC_{a}$ es 
que requieron intenso cómputo estadístico; de acuerdo a Effron y Tibshirani al
menos $B= 1000$ replicaciones son necesairas para reducir el error de muestreo. 
Ante esto surgen los intervalos ABC (**approximate bootstrap confidence intervals**), 
que es un método para aproximar $BC_{a}$ analíticamente (usando expansiones en 
Taylor).

Usando la implementación del paquete bootstrap:

```{r}
var_sesgada <- function(x) sum((x - mean(x)) ^ 2) / length(x)
bcanon(x = spatial[, 1], nboot = 2000, theta = var_sesgada)

b_var <- rerun(1000, var_sesgada(sample(spatial[, 1], size = length(spatial[, 1]), replace = TRUE))) %>% flatten_dbl()

qplot(b_var) + geom_histogram(binwidth = 10)

ggplot(data_frame(b_var)) +
    geom_abline(color = "red", alpha = 0.5) +
    stat_qq(aes(sample = b_var), 
        dparams = list(mean = mean(b_var), sd = sd(b_var))) +
  geom_abline()
```

**Ejercicio:** Comapara los intervalos del ejemplo anterior con los intervalos
normales y de percentiles.

Otros intervalos basados en bootstrap incluyen los intervalos pivotales y los 
intervalos bootstrap-t. Sin embargo, BC y ABC son mejores alternativas.

4. **Intervalos pivotales**. Sean $\theta=t(P)$ y $\hat{\theta}=t(P_n)$. Definimos
el pivote $R=\hat{\theta}-\theta$. Sea $H(r)$ la función de distribución 
acumulada del pivote:
$$H(r) = P(R<r)$$

Definimos el intervalo pivotal $C_n^*=(a,b)$ donde:
$$a=\hat{\theta}-H^{-1}(1-\alpha), b=\hat{\theta}-H^{-1}(\alpha)$$
$C_n^*$ es un intervalo de confianza de $1-2\alpha$ para $\theta$; sin
embargo, $a$ y $b$ dependen de la distribución desconocida $H$. Si estimamos 
$H(r)$ utilizando bootstrap:
$$\hat{H}(r)=\frac{1}{B}\sum_{b=1}^B I(R^*_b \le r)$$
obtenemos el estimador por intervalos:
$$\hat{C}_n^*=(2\hat{\theta} - \hat{\theta}_B^*(1-\alpha), 2\hat{\theta} + \hat{\theta}_B^*(\alpha))$$



##### Referencias
* An introduction to the bootstrap, B. Effron, R. J. Tibshirani. Tiene asociado
l paquete _bootstrap_ de R.
* All of statistics, L. Wasserman.  
* Bootstrap Methods and their applications, A. C. Davison, D.V. Hinkley. Tiene 
asociado el paquete _boot_ de R.
* Practical Tools for Designing and Weighting Survey Samples, R. Valliant, 
J.A. Dever, F. Kreuter.
